{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "477CXDX-_kbj"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "print(sys.version)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5PQBSosxAb-Q",
        "outputId": "221a94fc-7e2f-4525-9adb-370969a87dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q librosa==0.10.1 soundfile audioread numpy scipy scikit-learn joblib pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0EzFoHsIkpy",
        "outputId": "066692b1-2fdf-42fc-f9c2-d27ca6111909"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/253.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ\u001b[0m \u001b[32m245.8/253.7 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m253.7/253.7 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(\"data/audio/original\", exist_ok=True)\n",
        "os.makedirs(\"data/audio/prompt_injection\", exist_ok=True)\n",
        "os.makedirs(\"models\", exist_ok=True)\n"
      ],
      "metadata": {
        "id": "vcz8p7McKZBV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "\n",
        "audio_path = \"/content/original.m4a\"  # change name if needed\n",
        "\n",
        "y, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
        "\n",
        "print(\"Audio loaded\")\n",
        "print(\"Sample rate:\", sr)\n",
        "print(\"Duration:\", len(y)/sr)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDBmGjkaKq4N",
        "outputId": "f82fa6fa-7fbe-4d44-824d-5e1afd06481a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-4217415713.py:5: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio loaded\n",
            "Sample rate: 16000\n",
            "Duration: 48.0426875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def extract_features(audio_path):\n",
        "    y, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
        "    y = librosa.util.normalize(y)\n",
        "\n",
        "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
        "    delta = librosa.feature.delta(mfcc)\n",
        "    zcr = librosa.feature.zero_crossing_rate(y)\n",
        "    spectral_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
        "\n",
        "    features = np.hstack([\n",
        "        mfcc.mean(axis=1),\n",
        "        delta.mean(axis=1),\n",
        "        zcr.mean(),\n",
        "        spectral_bw.mean()\n",
        "    ])\n",
        "\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "_mc2ghseLQ9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = [], []\n",
        "\n",
        "for label, folder in enumerate([\"original\", \"prompt_injection\"]):\n",
        "    path = f\"data/audio/{folder}\"\n",
        "    for file in os.listdir(path):\n",
        "        if file.endswith((\".mp3\", \".m4a\", \".wav\")):\n",
        "            feat = extract_features(os.path.join(path, file))\n",
        "            X.append(feat)\n",
        "            y.append(label)\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "print(\"Samples:\", X.shape)\n",
        "print(\"Labels:\", set(y))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5JQr3SIMbFE",
        "outputId": "f5c8f5ce-370f-4789-9829-55f9241875ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2669740500.py:4: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/tmp/ipython-input-2669740500.py:4: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/tmp/ipython-input-2669740500.py:4: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  y, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:183: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Samples: (12, 28)\n",
            "Labels: {np.int64(0), np.int64(1)}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import joblib\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_scaled, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    max_depth=12,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "joblib.dump(model, \"models/audio_classifier.pkl\")\n",
        "joblib.dump(scaler, \"models/scaler.pkl\")\n",
        "\n",
        "print(\"Model trained & saved\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AI7p75RCMfAU",
        "outputId": "86e1b6e5-a83e-40a8-d5fc-4be0251a798e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model trained & saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def replay_score(audio_path):\n",
        "    y, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
        "    spec = np.abs(librosa.stft(y))\n",
        "\n",
        "    high = spec[int(0.75 * spec.shape[0]):].mean()\n",
        "    low = spec[:int(0.25 * spec.shape[0])].mean()\n",
        "\n",
        "    return float(high / (low + 1e-6))\n"
      ],
      "metadata": {
        "id": "IkX-aPHLM2yU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hidden_command_score(audio_path):\n",
        "    y, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
        "    spec = np.abs(librosa.stft(y))\n",
        "    freqs = librosa.fft_frequencies(sr=sr)\n",
        "\n",
        "    hf = spec[freqs > 7000].mean()\n",
        "    total = spec.mean()\n",
        "\n",
        "    return float(hf / (total + 1e-6))\n"
      ],
      "metadata": {
        "id": "YHQpB6jvM7Q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def assess_audio_risk(audio_path):\n",
        "    feat = extract_features(audio_path)\n",
        "    feat = scaler.transform([feat])\n",
        "\n",
        "    ml_prob = model.predict_proba(feat)[0][1]\n",
        "    replay_prob = min(replay_score(audio_path), 1.0)\n",
        "    hidden_prob = hidden_command_score(audio_path)\n",
        "\n",
        "    final_risk = (\n",
        "        0.6 * ml_prob +\n",
        "        0.2 * replay_prob +\n",
        "        0.2 * hidden_prob\n",
        "    )\n",
        "\n",
        "    decision = (\n",
        "        \"BLOCK\" if final_risk > 0.75 else\n",
        "        \"FLAG\" if final_risk > 0.45 else\n",
        "        \"ALLOW\"\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"risk_score\": round(final_risk, 3),\n",
        "        \"decision\": decision\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Es_RImflM9dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = assess_audio_risk(\"data/audio/prompt_injection/synthetic_1.mp3\")\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRX7dfxnNBEH",
        "outputId": "cff5e333-feb0-4044-f5a6-1661448d60a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'risk_score': np.float64(0.61), 'decision': 'FLAG'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch torchaudio librosa numpy matplotlib scikit-learn\n"
      ],
      "metadata": {
        "id": "whf3DrKeNC5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q numpy<2.0 librosa soundfile scipy scikit-learn torch openai-whisper tqdm\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPWSnE56OakF",
        "outputId": "4d94f895-901d-4c7a-cd8c-0d88fb7c281b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: 2.0: No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Config\n",
        "SAMPLE_RATE = 16000\n",
        "DURATION = 5\n",
        "N_MFCC = 40\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 30        # ‚Üë increase\n",
        "LR = 1e-3\n",
        "\n"
      ],
      "metadata": {
        "id": "-ufUQqwsOg0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_audio(path):\n",
        "    audio, sr = librosa.load(path, sr=SAMPLE_RATE, mono=True)\n",
        "    audio = librosa.util.fix_length(audio, size=SAMPLE_RATE * DURATION)\n",
        "    return audio\n"
      ],
      "metadata": {
        "id": "zm0oHWOkOqZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def extract_features(audio):\n",
        "    mfcc = librosa.feature.mfcc(\n",
        "        y=audio,\n",
        "        sr=SAMPLE_RATE,\n",
        "        n_mfcc=N_MFCC\n",
        "    )\n",
        "    delta = librosa.feature.delta(mfcc)\n",
        "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "\n",
        "    features = np.concatenate([\n",
        "        np.mean(mfcc, axis=1),\n",
        "        np.mean(delta, axis=1),\n",
        "        np.mean(delta2, axis=1)\n",
        "    ])\n",
        "\n",
        "    return features.astype(np.float32)  # shape = 120\n",
        "\n"
      ],
      "metadata": {
        "id": "fn5JbLVxOzGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example structure:\n",
        "# data/\n",
        "#   safe/\n",
        "#   scam/\n",
        "\n",
        "DATA_DIR = \"/content/\"\n",
        "\n",
        "AUDIO_EXTS = (\".wav\", \".mp3\", \".m4a\")\n",
        "\n",
        "audio_files = []\n",
        "labels = []\n",
        "\n",
        "for label, folder in enumerate([\"original\", \"prompt_injection\"]):\n",
        "    folder_path = os.path.join(DATA_DIR, folder)\n",
        "    for file in os.listdir(folder_path):\n",
        "        if file.lower().endswith(AUDIO_EXTS):\n",
        "            audio_files.append(os.path.join(folder_path, file))\n",
        "            labels.append(label)\n",
        "\n",
        "print(\"Total audio files:\", len(audio_files))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIyDiJUG7egr",
        "outputId": "754a4292-c039-4122-830a-668befd3e52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total audio files: 192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train_files, X_val_files, y_train, y_val = train_test_split(\n",
        "    audio_files,\n",
        "    labels,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=labels   # VERY IMPORTANT\n",
        ")\n"
      ],
      "metadata": {
        "id": "kwvJomJl7s5E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_feature_matrix(files):\n",
        "    feats = []\n",
        "    for path in files:\n",
        "        audio = load_audio(path)\n",
        "        feats.append(extract_features(audio))\n",
        "    return np.array(feats)\n"
      ],
      "metadata": {
        "id": "AdD6SuI57xe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = build_feature_matrix(X_train_files)\n",
        "X_val   = build_feature_matrix(X_val_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLE_QXiE70-l",
        "outputId": "75ec347c-a900-4bfb-fcec-63c4fbe0b093"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1496612483.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(path, sr=SAMPLE_RATE, mono=True)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/tmp/ipython-input-1496612483.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(path, sr=SAMPLE_RATE, mono=True)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/tmp/ipython-input-1496612483.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(path, sr=SAMPLE_RATE, mono=True)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/tmp/ipython-input-1496612483.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(path, sr=SAMPLE_RATE, mono=True)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/tmp/ipython-input-1496612483.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(path, sr=SAMPLE_RATE, mono=True)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/tmp/ipython-input-1496612483.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(path, sr=SAMPLE_RATE, mono=True)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n",
            "/tmp/ipython-input-1496612483.py:2: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio, sr = librosa.load(path, sr=SAMPLE_RATE, mono=True)\n",
            "/usr/local/lib/python3.12/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val   = scaler.transform(X_val)\n"
      ],
      "metadata": {
        "id": "Ga7O6bsy8BfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n"
      ],
      "metadata": {
        "id": "qfUvczi9PBNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class AudioClassifier(nn.Module):\n",
        "    def __init__(self, input_dim=120, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "\n",
        "            nn.Linear(256, 128),\n",
        "            nn.BatchNorm1d(128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "metadata": {
        "id": "902zK1DKQDln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = AudioDataset(X_train, y_train)\n",
        "val_ds   = AudioDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "L9L782kB9O_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IN6YTQmwQF19",
        "outputId": "78a7ee34-2a3f-4ea5-c2a7-32a65f4c0a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total audio files: 200\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "id": "SG1sd7C6RPbp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "counts = Counter(y_train)\n",
        "total = sum(counts.values())\n",
        "\n",
        "weights = [\n",
        "    total / counts[0],\n",
        "    total / counts[1]\n",
        "]\n",
        "\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n"
      ],
      "metadata": {
        "id": "7Is7WpBxi2J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# Device\n",
        "# =========================\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# =========================\n",
        "# Hyperparameters\n",
        "# =========================\n",
        "LR = 3e-4          # stable for small dataset\n",
        "EPOCHS = 20\n",
        "\n",
        "# =========================\n",
        "# Model\n",
        "# =========================\n",
        "model = AudioClassifier().to(device)\n",
        "\n",
        "# =========================\n",
        "# Loss (with class weights)\n",
        "# =========================\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "# =========================\n",
        "# Optimizer\n",
        "# =========================\n",
        "optimizer = torch.optim.AdamW(\n",
        "    model.parameters(),\n",
        "    lr=LR,\n",
        "    weight_decay=0.005\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# Training Loop\n",
        "# =========================\n",
        "for epoch in range(EPOCHS):\n",
        "    # ---- Train ----\n",
        "    model.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} [Train]\"):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(x)\n",
        "        loss = criterion(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    train_loss /= len(train_loader)\n",
        "\n",
        "    # ---- Validation ----\n",
        "    model.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x, y = x.to(device), y.to(device)\n",
        "\n",
        "            outputs = model(x)\n",
        "            loss = criterion(outputs, y)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            preds = torch.argmax(outputs, dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += y.size(0)\n",
        "\n",
        "    val_loss /= len(val_loader)\n",
        "    val_acc = correct / total\n",
        "\n",
        "    # ---- Logs ----\n",
        "    print(\n",
        "        f\"Epoch {epoch+1}/{EPOCHS} | \"\n",
        "        f\"Train Loss: {train_loss:.4f} | \"\n",
        "        f\"Val Loss: {val_loss:.4f} | \"\n",
        "        f\"Val Acc: {val_acc:.4f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xki2_x8sRSaI",
        "outputId": "8ac44bf5-34cf-4c6e-ce2b-c35c21c5f45c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 179.21it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 | Train Loss: 0.7124 | Val Loss: 0.6999 | Val Acc: 0.5641\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 195.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 | Train Loss: 0.6695 | Val Loss: 0.6899 | Val Acc: 0.5897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 205.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 | Train Loss: 0.6652 | Val Loss: 0.6790 | Val Acc: 0.6667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 240.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20 | Train Loss: 0.6223 | Val Loss: 0.6681 | Val Acc: 0.6667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 194.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20 | Train Loss: 0.6083 | Val Loss: 0.6581 | Val Acc: 0.6410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 257.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20 | Train Loss: 0.6026 | Val Loss: 0.6438 | Val Acc: 0.6923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 191.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20 | Train Loss: 0.5478 | Val Loss: 0.6328 | Val Acc: 0.7179\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 204.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20 | Train Loss: 0.5264 | Val Loss: 0.6320 | Val Acc: 0.6923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 188.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20 | Train Loss: 0.4900 | Val Loss: 0.6288 | Val Acc: 0.6923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 199.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20 | Train Loss: 0.4836 | Val Loss: 0.6256 | Val Acc: 0.6923\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 178.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20 | Train Loss: 0.4750 | Val Loss: 0.6300 | Val Acc: 0.6667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 173.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20 | Train Loss: 0.4694 | Val Loss: 0.6321 | Val Acc: 0.6667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 144.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20 | Train Loss: 0.4269 | Val Loss: 0.6323 | Val Acc: 0.6154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 158.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20 | Train Loss: 0.4039 | Val Loss: 0.6342 | Val Acc: 0.6154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 198.99it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20 | Train Loss: 0.4443 | Val Loss: 0.6329 | Val Acc: 0.6154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 204.42it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20 | Train Loss: 0.4026 | Val Loss: 0.6330 | Val Acc: 0.5897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 208.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20 | Train Loss: 0.3882 | Val Loss: 0.6234 | Val Acc: 0.6154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 182.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20 | Train Loss: 0.3923 | Val Loss: 0.6278 | Val Acc: 0.6154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 176.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20 | Train Loss: 0.3306 | Val Loss: 0.6319 | Val Acc: 0.6154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20 [Train]: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [00:00<00:00, 178.18it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20 | Train Loss: 0.3335 | Val Loss: 0.6389 | Val Acc: 0.6410\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for x, y in val_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        preds = torch.argmax(model(x), dim=1)\n",
        "        correct += (preds == y).sum().item()\n",
        "        total += y.size(0)\n",
        "\n",
        "print(\"Validation Accuracy:\", correct / total)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nfE9BfDR0Jp",
        "outputId": "858ab957-a404-4fd8-c270-d0ce1f2e5dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.6410256410256411\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Config\n",
        "SAMPLE_RATE = 16000\n",
        "DURATION = 5\n",
        "N_MFCC = 40\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 30        # ‚Üë increase\n",
        "LR = 1e-3\n",
        "\n"
      ],
      "metadata": {
        "id": "40yr7aDncOXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "oqphMXP7kncc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SAMPLE_RATE = 16000\n",
        "DURATION = 5\n",
        "N_MFCC = 40\n"
      ],
      "metadata": {
        "id": "oHLBzs4rko2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_audio(path):\n",
        "    audio, _ = librosa.load(path, sr=SAMPLE_RATE, mono=True)\n",
        "    audio = librosa.util.fix_length(\n",
        "        audio,\n",
        "        size=SAMPLE_RATE * DURATION\n",
        "    )\n",
        "    return audio\n"
      ],
      "metadata": {
        "id": "mLXBh4UVkoi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_features(audio):\n",
        "    mfcc = librosa.feature.mfcc(\n",
        "        y=audio,\n",
        "        sr=SAMPLE_RATE,\n",
        "        n_mfcc=N_MFCC\n",
        "    )\n",
        "    delta = librosa.feature.delta(mfcc)\n",
        "    delta2 = librosa.feature.delta(mfcc, order=2)\n",
        "\n",
        "    features = np.concatenate([\n",
        "        np.mean(mfcc, axis=1),\n",
        "        np.mean(delta, axis=1),\n",
        "        np.mean(delta2, axis=1)\n",
        "    ])\n",
        "\n",
        "    return features.astype(np.float32)\n"
      ],
      "metadata": {
        "id": "4B96fz8ZkvZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def test_audio(audio_path, model, scaler, device):\n",
        "    model.eval()\n",
        "\n",
        "    try:\n",
        "        audio = load_audio(audio_path)\n",
        "        features = extract_features(audio)\n",
        "        features = scaler.transform([features])[0]\n",
        "\n",
        "        x = torch.tensor(features, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            probs = torch.softmax(model(x), dim=1)[0]\n",
        "            orig_prob = probs[0].item()\n",
        "            inj_prob  = probs[1].item()\n",
        "\n",
        "        # üî• ORIGINAL-favoring thresholds\n",
        "        PI_THRESH   = 0.75\n",
        "        ORIG_THRESH = 0.40\n",
        "\n",
        "        if inj_prob >= PI_THRESH:\n",
        "            final_label = \"PROMPT_INJECTION\"\n",
        "            confidence = inj_prob\n",
        "        elif orig_prob > ORIG_THRESH:\n",
        "            final_label = \"ORIGINAL\"\n",
        "            confidence = orig_prob\n",
        "        else:\n",
        "            # soft bias toward ORIGINAL\n",
        "            final_label = \"ORIGINAL\"\n",
        "            confidence = orig_prob\n",
        "\n",
        "\n",
        "        print(\"=\" * 55)\n",
        "        print(\"Audio file  :\", audio_path)\n",
        "        print(\"Prediction  :\", final_label)\n",
        "        print(f\"Confidence  : {confidence * 100:.2f}%\")\n",
        "        print(\"=\" * 55)\n",
        "\n",
        "        return final_label, confidence\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"‚ùå Error testing audio:\", audio_path)\n",
        "        print(\"Reason:\", str(e))\n",
        "        return None, None\n"
      ],
      "metadata": {
        "id": "WC46z8TqkyPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_audio_smooth(audio_path, model, scaler, device, runs=5):\n",
        "    probs = []\n",
        "\n",
        "    for _ in range(runs):\n",
        "        audio = load_audio(audio_path)\n",
        "        audio = audio + 0.01 * np.random.randn(len(audio))  # tiny noise\n",
        "\n",
        "        features = extract_features(audio)\n",
        "        features = scaler.transform([features])[0]\n",
        "\n",
        "        x = torch.tensor(\n",
        "            features,\n",
        "            dtype=torch.float32\n",
        "        ).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # probability of PROMPT_INJECTION\n",
        "            p = torch.softmax(model(x), dim=1)[0, 1].item()\n",
        "            probs.append(p)\n",
        "\n",
        "    avg_prob = float(np.mean(probs))   # <-- this is inj_prob (smoothed)\n",
        "    orig_prob = 1 - avg_prob\n",
        "\n",
        "    # üî• ORIGINAL-favoring thresholds\n",
        "    PI_THRESH   = 0.75\n",
        "    ORIG_THRESH = 0.50\n",
        "\n",
        "    if avg_prob >= PI_THRESH:\n",
        "        final_label = \"PROMPT_INJECTION\"\n",
        "        confidence = avg_prob\n",
        "    elif orig_prob > ORIG_THRESH:\n",
        "        final_label = \"ORIGINAL\"\n",
        "        confidence = orig_prob\n",
        "    else:\n",
        "        # soft bias toward ORIGINAL\n",
        "        final_label = \"ORIGINAL\"\n",
        "        confidence = orig_prob\n",
        "\n",
        "    print(\"=\" * 55)\n",
        "    print(\"Audio file :\", audio_path)\n",
        "    print(\"Avg Prob (PI):\", f\"{avg_prob:.2f}\")\n",
        "    print(\"Prediction :\", final_label)\n",
        "    print(f\"Confidence : {confidence * 100:.2f}%\")\n",
        "    print(\"=\" * 55)\n",
        "\n",
        "    return final_label, confidence\n"
      ],
      "metadata": {
        "id": "a-a7k36cqbWA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_audio_smooth(\n",
        "    \"/content/prompt_injection/synthetic (45).wav\",\n",
        "    model,\n",
        "    scaler,\n",
        "    device\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bTTK4Rxk1qz",
        "outputId": "a0b272f0-dc33-4d5c-87ea-8f7f124f3ac2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======================================================\n",
            "Audio file : /content/prompt_injection/synthetic (45).wav\n",
            "Avg Prob (PI): 0.84\n",
            "Prediction : PROMPT_INJECTION\n",
            "Confidence : 84.26%\n",
            "=======================================================\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('PROMPT_INJECTION', 0.8426075339317322)"
            ]
          },
          "metadata": {},
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# TRAINING FINISHED\n",
        "# =========================\n",
        "\n",
        "print(\"‚úÖ Training completed\")\n",
        "\n",
        "# =========================\n",
        "# SAVE MODEL FOR FUTURE USE\n",
        "# =========================\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import joblib\n",
        "\n",
        "SAVE_DIR = \"final_saved_model\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# 1Ô∏è‚É£ Save model weights\n",
        "torch.save(model.state_dict(), f\"{SAVE_DIR}/audio_classifier.pt\")\n",
        "\n",
        "# 2Ô∏è‚É£ Save scaler (VERY IMPORTANT)\n",
        "joblib.dump(scaler, f\"{SAVE_DIR}/scaler.pkl\")\n",
        "\n",
        "# 3Ô∏è‚É£ Save config\n",
        "config = {\n",
        "    \"input_dim\": 120,   # MFCC + delta + delta2\n",
        "    \"num_classes\": 2\n",
        "}\n",
        "torch.save(config, f\"{SAVE_DIR}/config.pt\")\n",
        "\n",
        "print(\"‚úÖ Model, scaler, and config saved successfully\")\n",
        "print(f\"üìÅ Files saved inside: {SAVE_DIR}/\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fqK4aYlNDeUx",
        "outputId": "9dc30d9b-b69d-488a-adc4-b59fd2a04e6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Training completed\n",
            "‚úÖ Model, scaler, and config saved successfully\n",
            "üìÅ Files saved inside: final_saved_model/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick sanity check\n",
        "model.load_state_dict(torch.load(\"saved_model/audio_classifier.pt\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "print(\"‚úÖ Reload test passed\")\n"
      ],
      "metadata": {
        "id": "9BW1BDEzDveS",
        "outputId": "df1a97af-99cc-4a35-e9f4-39aca120f83c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Reload test passed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "TEST_DIR = \"/content/test_audios\"\n",
        "AUDIO_EXTS = (\".wav\", \".mp3\", \".m4a\")\n",
        "\n",
        "for file in os.listdir(TEST_DIR):\n",
        "    if file.lower().endswith(AUDIO_EXTS):\n",
        "        test_audio(\n",
        "            os.path.join(TEST_DIR, file),\n",
        "            model,\n",
        "            scaler,\n",
        "            device\n",
        "        )\n"
      ],
      "metadata": {
        "id": "KIO-H8tBk1Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U openai-whisper\n",
        "!apt-get update -y\n",
        "!apt-get install -y ffmpeg\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4bmMFWP15Gv",
        "outputId": "58d7d995-c2fd-4eec-c157-bfd3f73f4bef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/803.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m798.7/803.2 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.9.0+cpu)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.2)\n",
            "Collecting triton>=2 (from openai-whisper)\n",
            "  Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2026.1.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.3)\n",
            "Downloading triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (188.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=285f25cc158a7ba9ed2a41ecd0758c197ba58d76a9911c0f221c4ae7c89b13a7\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: triton, openai-whisper\n",
            "Successfully installed openai-whisper-20250625 triton-3.6.0\n",
            "Get:1 https://cli.github.com/packages stable InRelease [3,917 B]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:4 https://cli.github.com/packages stable/main amd64 Packages [356 B]\n",
            "Get:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,632 B]\n",
            "Get:6 https://r2u.stat.illinois.edu/ubuntu jammy InRelease [6,555 B]\n",
            "Get:7 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:9 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [83.8 kB]\n",
            "Get:10 https://r2u.stat.illinois.edu/ubuntu jammy/main amd64 Packages [2,892 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [70.9 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [4,014 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:14 https://r2u.stat.illinois.edu/ubuntu jammy/main all Packages [9,712 kB]\n",
            "Get:15 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease [24.6 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [3,677 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [6,607 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [38.8 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,608 kB]\n",
            "Get:20 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy/main amd64 Packages [75.3 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [6,388 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,297 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [62.6 kB]\n",
            "Fetched 37.0 MB in 5s (6,791 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 53 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "whisper_model = whisper.load_model(\"small\")\n",
        "\n",
        "def transcribe_audio(path):\n",
        "    result = whisper_model.transcribe(path)\n",
        "    return result[\"text\"]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTEhQfHP1ZpT",
        "outputId": "910d2af8-2e70-451c-b586-28d0588a143a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 461M/461M [00:04<00:00, 110MiB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_audio = audio_files[0]\n",
        "\n",
        "audio = load_audio(test_audio)\n",
        "features = extract_mfcc(audio)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    logits = model(torch.tensor(features).unsqueeze(0).to(device))\n",
        "    prediction = torch.argmax(logits, dim=1).item()\n",
        "\n",
        "print(\"Prediction:\", \"SCAM\" if prediction == 1 else \"SAFE\")\n",
        "print(\"Transcript:\", transcribe_audio(test_audio))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ2KuKXt1eXu",
        "outputId": "e13e4c8e-49d2-4183-c02a-65bf08949dea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction: SAFE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcript:  let's say your rectangle which has a curve instead of coming to a point.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JfnhSryh0yUh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}