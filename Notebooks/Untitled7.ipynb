{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXXri9rTZYde"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_DIR = \"/content/images\"\n",
        "CSV_PATH = \"/content/10_Labels.csv\"\n",
        "\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 5\n",
        "LR = 1e-4\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_yNdFquaIYC",
        "outputId": "3f7eb41d-6884-4769-85b5-09a03fdec61f"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n"
      ],
      "metadata": {
        "id": "cUkVcFX2fveD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = AdversarialImageDataset(\n",
        "    img_dir=IMG_DIR,\n",
        "    csv_path=CSV_PATH,\n",
        "    transform=transform\n",
        ")\n",
        "\n",
        "print(\"Total samples:\", len(dataset))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW4_Maatf4kO",
        "outputId": "4b7e0aae-a8fb-4c5f-8b0e-417ffc1c4642"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total samples: 3925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "folder_path = '/content/images/'\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(('.jpg', '.jpeg', '.png')):\n",
        "        img_path = os.path.join(folder_path, filename)\n",
        "        with Image.open(img_path) as img:\n",
        "            width, height = img.size\n",
        "            print(f\"{filename}: {width}x{height}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "reLSe1KBqQdw",
        "outputId": "997bb1d4-306c-4816-caf2-31568e5b0102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 121.jpg: 231x231\n",
            " 86.jpg: 231x231\n",
            " 105.jpg: 231x231\n",
            " 51.jpg: 231x231\n",
            " 125.jpg: 231x231\n",
            " 57.jpg: 231x231\n",
            " 14.jpg: 231x231\n",
            " 52.jpg: 231x231\n",
            " 36.jpg: 231x231\n",
            " 48.jpg: 231x231\n",
            " 50.jpg: 231x231\n",
            " 12.jpg: 231x231\n",
            " 133.jpg: 231x231\n",
            " 76.jpg: 231x231\n",
            " 59.jpg: 231x231\n",
            " 78.jpg: 231x231\n",
            " 17.jpg: 231x231\n",
            " 150.jpg: 231x231\n",
            " 130.jpg: 231x231\n",
            " 35.jpg: 231x231\n",
            " 91.jpg: 231x231\n",
            " 27.jpg: 231x231\n",
            " 89.jpg: 231x231\n",
            " 143.jpg: 231x231\n",
            " 103.jpg: 231x231\n",
            " 8.jpg: 231x231\n",
            " 147.jpg: 231x231\n",
            " 40.jpg: 231x231\n",
            " 123.jpg: 231x231\n",
            " 127.jpg: 231x231\n",
            " 38.jpg: 231x231\n",
            " 20.jpg: 231x231\n",
            " 58.jpg: 231x231\n",
            " 18.jpg: 231x231\n",
            " 136.jpg: 231x231\n",
            " 94.jpg: 231x231\n",
            " 104.jpg: 231x231\n",
            " 128.jpg: 231x231\n",
            " 156.jpg: 231x231\n",
            " 148.jpg: 231x231\n",
            " 152.jpg: 231x231\n",
            " 107.jpg: 231x231\n",
            " 149.jpg: 231x231\n",
            " 129.jpg: 231x231\n",
            " 84.jpg: 231x231\n",
            " 154.jpg: 231x231\n",
            " 146.jpg: 231x231\n",
            " 142.jpg: 231x231\n",
            " 110.jpg: 231x231\n",
            " 80.jpg: 231x231\n",
            " 139.jpg: 231x231\n",
            " 61.jpg: 231x231\n",
            " 85.jpg: 231x231\n",
            " 79.jpg: 231x231\n",
            " 73.jpg: 231x231\n",
            " 71.jpg: 231x231\n",
            " 157.jpg: 231x231\n",
            " 132.jpg: 231x231\n",
            " 39.jpg: 231x231\n",
            " 70.jpg: 231x231\n",
            " 111.jpg: 231x231\n",
            " 69.jpg: 231x231\n",
            " 135.jpg: 231x231\n",
            " 124.jpg: 231x231\n",
            " 131.jpg: 231x231\n",
            " 140.jpg: 231x231\n",
            " 34.jpg: 231x231\n",
            " 23.jpg: 231x231\n",
            " 26.jpg: 231x231\n",
            " 141.jpg: 231x231\n",
            " 159.jpg: 231x231\n",
            " 119.jpg: 231x231\n",
            " 97.jpg: 231x231\n",
            " 0.jpg: 231x231\n",
            " 117.jpg: 231x231\n",
            " 113.jpg: 231x231\n",
            " 114.jpg: 231x231\n",
            " 62.jpg: 231x231\n",
            " 77.jpg: 231x231\n",
            " 134.jpg: 231x231\n",
            " 11.jpg: 231x231\n",
            " 98.jpg: 231x231\n",
            " 120.jpg: 231x231\n",
            " 116.jpg: 231x231\n",
            " 68.jpg: 231x231\n",
            " 29.jpg: 231x231\n",
            " 101.jpg: 231x231\n",
            " 25.jpg: 231x231\n",
            " 106.jpg: 231x231\n",
            " 32.jpg: 231x231\n",
            " 112.jpg: 231x231\n",
            " 15.jpg: 231x231\n",
            " 44.jpg: 231x231\n",
            " 55.jpg: 231x231\n",
            " 60.jpg: 231x231\n",
            " 24.jpg: 231x231\n",
            " 95.jpg: 231x231\n",
            " 13.jpg: 231x231\n",
            " 153.jpg: 231x231\n",
            " 137.jpg: 231x231\n",
            " 46.jpg: 231x231\n",
            " 21.jpg: 231x231\n",
            " 5.jpg: 231x231\n",
            " 144.jpg: 231x231\n",
            " 7.jpg: 231x231\n",
            " 118.jpg: 231x231\n",
            " 155.jpg: 231x231\n",
            " 53.jpg: 231x231\n",
            " 16.jpg: 231x231\n",
            " 49.jpg: 231x231\n",
            " 158.jpg: 231x231\n",
            " 45.jpg: 231x231\n",
            " 9.jpg: 231x231\n",
            " 42.jpg: 231x231\n",
            " 63.jpg: 231x231\n",
            " 31.jpg: 231x231\n",
            " 47.jpg: 231x231\n",
            " 67.jpg: 231x231\n",
            " 33.jpg: 231x231\n",
            " 74.jpg: 231x231\n",
            " 96.jpg: 231x231\n",
            " 90.jpg: 231x231\n",
            " 82.jpg: 231x231\n",
            " 41.jpg: 231x231\n",
            " 126.jpg: 231x231\n",
            " 108.jpg: 231x231\n",
            " 122.jpg: 231x231\n",
            " 3.jpg: 231x231\n",
            " 65.jpg: 231x231\n",
            " 2.jpg: 231x231\n",
            " 28.jpg: 231x231\n",
            " 56.jpg: 231x231\n",
            " 30.jpg: 231x231\n",
            " 93.jpg: 231x231\n",
            " 87.jpg: 231x231\n",
            " 109.jpg: 231x231\n",
            " 83.jpg: 231x231\n",
            " 81.jpg: 231x231\n",
            " 102.jpg: 231x231\n",
            " 4.jpg: 231x231\n",
            " 75.jpg: 231x231\n",
            " 10.jpg: 231x231\n",
            " 115.jpg: 231x231\n",
            " 88.jpg: 231x231\n",
            " 43.jpg: 231x231\n",
            " 72.jpg: 231x231\n",
            " 64.jpg: 231x231\n",
            " 54.jpg: 231x231\n",
            " 99.jpg: 231x231\n",
            " 145.jpg: 231x231\n",
            " 6.jpg: 231x231\n",
            " 100.jpg: 231x231\n",
            " 138.jpg: 231x231\n",
            " 37.jpg: 231x231\n",
            " 92.jpg: 231x231\n",
            " 19.jpg: 231x231\n",
            " 22.jpg: 231x231\n",
            " 66.jpg: 231x231\n",
            " 151.jpg: 231x231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import os\n",
        "from torchvision import transforms\n",
        "\n",
        "class AdversarialImageDataset(Dataset):\n",
        "    def __init__(self, csv_path, image_dir):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.image_dir = image_dir\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),   # handles different image sizes\n",
        "            transforms.ToTensor()\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "\n",
        "        # âœ… CSV already contains correct filename (e.g. \"2980.jpg\")\n",
        "        img_name = str(row[\"image\"]).strip()\n",
        "\n",
        "        img_path = os.path.join(self.image_dir, img_name)\n",
        "\n",
        "        if not os.path.exists(img_path):\n",
        "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        label = int(row[\"adv\"])  # 0 = clean, 1 = adversarial\n",
        "\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "Ht6aPZzhgF_A"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = AdversarialImageDataset(\n",
        "    csv_path=\"/content/10_Labels.csv\",\n",
        "    image_dir=\"/content/images\"\n",
        ")\n",
        "\n",
        "img, label = dataset[0]\n",
        "print(\"Image shape:\", img.shape)\n",
        "print(\"Label:\", label)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGobCaWXtKpH",
        "outputId": "7cff5097-849b-4515-a848-9b19fe4eeabb"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image shape: torch.Size([3, 224, 224])\n",
            "Label: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check disk\n",
        "print(\"Files on disk (first 5):\", os.listdir(\"/content/images\")[:5])\n",
        "\n",
        "# Check CSV\n",
        "df = pd.read_csv(\"/content/10_Labels.csv\")\n",
        "print(\"Files in CSV (first 5):\", df[\"image\"].head().tolist())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJomBcW7t-53",
        "outputId": "484e2096-7c6a-44ee-ce79-eccc9d8e821c"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files on disk (first 5): [' 121.jpg', ' 86.jpg', ' 105.jpg', ' 51.jpg', ' 125.jpg']\n",
            "Files in CSV (first 5): ['0.jpg', '1.jpg', '2.jpg', '3.jpg', '4.jpg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "image_dir = \"/content/images\"\n",
        "\n",
        "for filename in os.listdir(image_dir):\n",
        "    # .strip() removes spaces from the beginning and end\n",
        "    clean_name = filename.strip()\n",
        "\n",
        "    if clean_name != filename:\n",
        "        old_path = os.path.join(image_dir, filename)\n",
        "        new_path = os.path.join(image_dir, clean_name)\n",
        "\n",
        "        # Rename the file on disk\n",
        "        os.rename(old_path, new_path)\n",
        "        print(f\"Fixed: '{filename}' -> '{clean_name}'\")\n",
        "\n",
        "print(\"\\nCleanup complete. Try running your Dataset code again!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XvbhRB0UuL0g",
        "outputId": "2c4c35e3-89e1-451c-84a3-9017dbf836f4"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fixed: ' 121.jpg' -> '121.jpg'\n",
            "Fixed: ' 86.jpg' -> '86.jpg'\n",
            "Fixed: ' 105.jpg' -> '105.jpg'\n",
            "Fixed: ' 51.jpg' -> '51.jpg'\n",
            "Fixed: ' 125.jpg' -> '125.jpg'\n",
            "Fixed: ' 57.jpg' -> '57.jpg'\n",
            "Fixed: ' 14.jpg' -> '14.jpg'\n",
            "Fixed: ' 52.jpg' -> '52.jpg'\n",
            "Fixed: ' 36.jpg' -> '36.jpg'\n",
            "Fixed: ' 48.jpg' -> '48.jpg'\n",
            "Fixed: ' 50.jpg' -> '50.jpg'\n",
            "Fixed: ' 12.jpg' -> '12.jpg'\n",
            "Fixed: ' 133.jpg' -> '133.jpg'\n",
            "Fixed: ' 76.jpg' -> '76.jpg'\n",
            "Fixed: ' 59.jpg' -> '59.jpg'\n",
            "Fixed: ' 78.jpg' -> '78.jpg'\n",
            "Fixed: ' 17.jpg' -> '17.jpg'\n",
            "Fixed: ' 150.jpg' -> '150.jpg'\n",
            "Fixed: ' 130.jpg' -> '130.jpg'\n",
            "Fixed: ' 35.jpg' -> '35.jpg'\n",
            "Fixed: ' 91.jpg' -> '91.jpg'\n",
            "Fixed: ' 27.jpg' -> '27.jpg'\n",
            "Fixed: ' 89.jpg' -> '89.jpg'\n",
            "Fixed: ' 143.jpg' -> '143.jpg'\n",
            "Fixed: ' 103.jpg' -> '103.jpg'\n",
            "Fixed: ' 8.jpg' -> '8.jpg'\n",
            "Fixed: ' 147.jpg' -> '147.jpg'\n",
            "Fixed: ' 40.jpg' -> '40.jpg'\n",
            "Fixed: ' 123.jpg' -> '123.jpg'\n",
            "Fixed: ' 127.jpg' -> '127.jpg'\n",
            "Fixed: ' 38.jpg' -> '38.jpg'\n",
            "Fixed: ' 20.jpg' -> '20.jpg'\n",
            "Fixed: ' 58.jpg' -> '58.jpg'\n",
            "Fixed: ' 18.jpg' -> '18.jpg'\n",
            "Fixed: ' 136.jpg' -> '136.jpg'\n",
            "Fixed: ' 94.jpg' -> '94.jpg'\n",
            "Fixed: ' 104.jpg' -> '104.jpg'\n",
            "Fixed: ' 128.jpg' -> '128.jpg'\n",
            "Fixed: ' 156.jpg' -> '156.jpg'\n",
            "Fixed: ' 148.jpg' -> '148.jpg'\n",
            "Fixed: ' 152.jpg' -> '152.jpg'\n",
            "Fixed: ' 107.jpg' -> '107.jpg'\n",
            "Fixed: ' 149.jpg' -> '149.jpg'\n",
            "Fixed: ' 129.jpg' -> '129.jpg'\n",
            "Fixed: ' 84.jpg' -> '84.jpg'\n",
            "Fixed: ' 154.jpg' -> '154.jpg'\n",
            "Fixed: ' 146.jpg' -> '146.jpg'\n",
            "Fixed: ' 142.jpg' -> '142.jpg'\n",
            "Fixed: ' 110.jpg' -> '110.jpg'\n",
            "Fixed: ' 80.jpg' -> '80.jpg'\n",
            "Fixed: ' 139.jpg' -> '139.jpg'\n",
            "Fixed: ' 61.jpg' -> '61.jpg'\n",
            "Fixed: ' 85.jpg' -> '85.jpg'\n",
            "Fixed: ' 79.jpg' -> '79.jpg'\n",
            "Fixed: ' 73.jpg' -> '73.jpg'\n",
            "Fixed: ' 71.jpg' -> '71.jpg'\n",
            "Fixed: ' 157.jpg' -> '157.jpg'\n",
            "Fixed: ' 132.jpg' -> '132.jpg'\n",
            "Fixed: ' 39.jpg' -> '39.jpg'\n",
            "Fixed: ' 70.jpg' -> '70.jpg'\n",
            "Fixed: ' 111.jpg' -> '111.jpg'\n",
            "Fixed: ' 69.jpg' -> '69.jpg'\n",
            "Fixed: ' 135.jpg' -> '135.jpg'\n",
            "Fixed: ' 124.jpg' -> '124.jpg'\n",
            "Fixed: ' 131.jpg' -> '131.jpg'\n",
            "Fixed: ' 140.jpg' -> '140.jpg'\n",
            "Fixed: ' 34.jpg' -> '34.jpg'\n",
            "Fixed: ' 23.jpg' -> '23.jpg'\n",
            "Fixed: ' 26.jpg' -> '26.jpg'\n",
            "Fixed: ' 141.jpg' -> '141.jpg'\n",
            "Fixed: ' 159.jpg' -> '159.jpg'\n",
            "Fixed: ' 119.jpg' -> '119.jpg'\n",
            "Fixed: ' 97.jpg' -> '97.jpg'\n",
            "Fixed: ' 0.jpg' -> '0.jpg'\n",
            "Fixed: ' 117.jpg' -> '117.jpg'\n",
            "Fixed: ' 113.jpg' -> '113.jpg'\n",
            "Fixed: ' 114.jpg' -> '114.jpg'\n",
            "Fixed: ' 62.jpg' -> '62.jpg'\n",
            "Fixed: ' 77.jpg' -> '77.jpg'\n",
            "Fixed: ' 134.jpg' -> '134.jpg'\n",
            "Fixed: ' 11.jpg' -> '11.jpg'\n",
            "Fixed: ' 98.jpg' -> '98.jpg'\n",
            "Fixed: ' 120.jpg' -> '120.jpg'\n",
            "Fixed: ' 116.jpg' -> '116.jpg'\n",
            "Fixed: ' 68.jpg' -> '68.jpg'\n",
            "Fixed: ' 29.jpg' -> '29.jpg'\n",
            "Fixed: ' 101.jpg' -> '101.jpg'\n",
            "Fixed: ' 25.jpg' -> '25.jpg'\n",
            "Fixed: ' 106.jpg' -> '106.jpg'\n",
            "Fixed: ' 32.jpg' -> '32.jpg'\n",
            "Fixed: ' 112.jpg' -> '112.jpg'\n",
            "Fixed: ' 15.jpg' -> '15.jpg'\n",
            "Fixed: ' 44.jpg' -> '44.jpg'\n",
            "Fixed: ' 55.jpg' -> '55.jpg'\n",
            "Fixed: ' 60.jpg' -> '60.jpg'\n",
            "Fixed: ' 24.jpg' -> '24.jpg'\n",
            "Fixed: ' 95.jpg' -> '95.jpg'\n",
            "Fixed: ' 13.jpg' -> '13.jpg'\n",
            "Fixed: ' 153.jpg' -> '153.jpg'\n",
            "Fixed: ' 137.jpg' -> '137.jpg'\n",
            "Fixed: ' 46.jpg' -> '46.jpg'\n",
            "Fixed: ' 21.jpg' -> '21.jpg'\n",
            "Fixed: ' 5.jpg' -> '5.jpg'\n",
            "Fixed: ' 144.jpg' -> '144.jpg'\n",
            "Fixed: ' 7.jpg' -> '7.jpg'\n",
            "Fixed: ' 118.jpg' -> '118.jpg'\n",
            "Fixed: ' 155.jpg' -> '155.jpg'\n",
            "Fixed: ' 53.jpg' -> '53.jpg'\n",
            "Fixed: ' 16.jpg' -> '16.jpg'\n",
            "Fixed: ' 49.jpg' -> '49.jpg'\n",
            "Fixed: ' 158.jpg' -> '158.jpg'\n",
            "Fixed: ' 45.jpg' -> '45.jpg'\n",
            "Fixed: ' 9.jpg' -> '9.jpg'\n",
            "Fixed: ' 42.jpg' -> '42.jpg'\n",
            "Fixed: ' 63.jpg' -> '63.jpg'\n",
            "Fixed: ' 31.jpg' -> '31.jpg'\n",
            "Fixed: ' 47.jpg' -> '47.jpg'\n",
            "Fixed: ' 67.jpg' -> '67.jpg'\n",
            "Fixed: ' 33.jpg' -> '33.jpg'\n",
            "Fixed: ' 74.jpg' -> '74.jpg'\n",
            "Fixed: ' 96.jpg' -> '96.jpg'\n",
            "Fixed: ' 90.jpg' -> '90.jpg'\n",
            "Fixed: ' 82.jpg' -> '82.jpg'\n",
            "Fixed: ' 41.jpg' -> '41.jpg'\n",
            "Fixed: ' 126.jpg' -> '126.jpg'\n",
            "Fixed: ' 108.jpg' -> '108.jpg'\n",
            "Fixed: ' 122.jpg' -> '122.jpg'\n",
            "Fixed: ' 3.jpg' -> '3.jpg'\n",
            "Fixed: ' 65.jpg' -> '65.jpg'\n",
            "Fixed: ' 2.jpg' -> '2.jpg'\n",
            "Fixed: ' 28.jpg' -> '28.jpg'\n",
            "Fixed: ' 56.jpg' -> '56.jpg'\n",
            "Fixed: ' 30.jpg' -> '30.jpg'\n",
            "Fixed: ' 93.jpg' -> '93.jpg'\n",
            "Fixed: ' 87.jpg' -> '87.jpg'\n",
            "Fixed: ' 109.jpg' -> '109.jpg'\n",
            "Fixed: ' 83.jpg' -> '83.jpg'\n",
            "Fixed: ' 81.jpg' -> '81.jpg'\n",
            "Fixed: ' 102.jpg' -> '102.jpg'\n",
            "Fixed: ' 4.jpg' -> '4.jpg'\n",
            "Fixed: ' 75.jpg' -> '75.jpg'\n",
            "Fixed: ' 10.jpg' -> '10.jpg'\n",
            "Fixed: ' 115.jpg' -> '115.jpg'\n",
            "Fixed: ' 88.jpg' -> '88.jpg'\n",
            "Fixed: ' 43.jpg' -> '43.jpg'\n",
            "Fixed: ' 72.jpg' -> '72.jpg'\n",
            "Fixed: ' 64.jpg' -> '64.jpg'\n",
            "Fixed: ' 54.jpg' -> '54.jpg'\n",
            "Fixed: ' 99.jpg' -> '99.jpg'\n",
            "Fixed: ' 145.jpg' -> '145.jpg'\n",
            "Fixed: ' 6.jpg' -> '6.jpg'\n",
            "Fixed: ' 100.jpg' -> '100.jpg'\n",
            "Fixed: ' 138.jpg' -> '138.jpg'\n",
            "Fixed: ' 37.jpg' -> '37.jpg'\n",
            "Fixed: ' 92.jpg' -> '92.jpg'\n",
            "Fixed: ' 19.jpg' -> '19.jpg'\n",
            "Fixed: ' 22.jpg' -> '22.jpg'\n",
            "Fixed: ' 66.jpg' -> '66.jpg'\n",
            "Fixed: ' 151.jpg' -> '151.jpg'\n",
            "\n",
            "Cleanup complete. Try running your Dataset code again!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "\n",
        "image_dir = \"/content/images\"\n",
        "\n",
        "# 1. Get all files and clean hidden spaces/parentheses first\n",
        "files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "# 2. Sort files numerically\n",
        "# (This ensures 2.jpg comes before 10.jpg)\n",
        "def extract_number(f):\n",
        "    s = re.findall(r'\\d+', f)\n",
        "    return int(s[0]) if s else 0\n",
        "\n",
        "files.sort(key=extract_number)\n",
        "\n",
        "# 3. Rename them to match the 0, 1, 2... sequence\n",
        "print(f\"Starting rename of {len(files)} files...\")\n",
        "\n",
        "for index, filename in enumerate(files):\n",
        "    old_path = os.path.join(image_dir, filename)\n",
        "\n",
        "    # Create the new name based on the index (0.jpg, 1.jpg, etc.)\n",
        "    new_name = f\"{index}.jpg\"\n",
        "    new_path = os.path.join(image_dir, new_name)\n",
        "\n",
        "    # Rename\n",
        "    os.rename(old_path, new_path)\n",
        "    if index < 5: # Print first 5 to verify\n",
        "        print(f\"Renamed: {filename} -> {new_name}\")\n",
        "\n",
        "print(\"Success! Your images are now 0.jpg, 1.jpg, 2.jpg...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QSgpE_cwjYC",
        "outputId": "1fbb8d61-e9e2-48ed-b077-8bd9b32578e6"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting rename of 159 files...\n",
            "Renamed: 0.jpg -> 0.jpg\n",
            "Renamed: 2.jpg -> 1.jpg\n",
            "Renamed: 3.jpg -> 2.jpg\n",
            "Renamed: 4.jpg -> 3.jpg\n",
            "Renamed: 5.jpg -> 4.jpg\n",
            "Success! Your images are now 0.jpg, 1.jpg, 2.jpg...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Paths\n",
        "image_dir = \"/content/images\"\n",
        "csv_path = \"/content/10_Labels.csv\"\n",
        "\n",
        "# 1. Get and Sort files numerically\n",
        "def extract_number(f):\n",
        "    s = re.findall(r'\\d+', f)\n",
        "    return int(s[0]) if s else 0\n",
        "\n",
        "files = [f for f in os.listdir(image_dir) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "files.sort(key=extract_number)\n",
        "\n",
        "# 2. Rename files on disk to 0.jpg, 1.jpg, etc.\n",
        "print(f\"Renaming {len(files)} images...\")\n",
        "for index, filename in enumerate(files):\n",
        "    old_path = os.path.join(image_dir, filename)\n",
        "    new_name = f\"{index}.jpg\"\n",
        "    new_path = os.path.join(image_dir, new_name)\n",
        "    os.rename(old_path, new_path)\n",
        "\n",
        "# 3. Update the CSV file\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "# Ensure the CSV length matches our image count\n",
        "# We only take the first 159 rows to match your 159 photos\n",
        "df = df.iloc[:len(files)].copy()\n",
        "\n",
        "# Update the 'image' column to match the new names (0.jpg, 1.jpg...)\n",
        "df['image'] = [f\"{i}.jpg\" for i in range(len(files))]\n",
        "\n",
        "# Save the corrected CSV\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\"Success! CSV updated. Total rows: {len(df)}\")\n",
        "print(\"First 5 rows of updated CSV:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ot-m0AbRxyGW",
        "outputId": "2f60094a-6688-4568-a340-f16b53f15f2c"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Renaming 159 images...\n",
            "Success! CSV updated. Total rows: 159\n",
            "First 5 rows of updated CSV:\n",
            "   Unnamed: 0  image  adv   actual_class\n",
            "0           0  0.jpg    0  garbage_truck\n",
            "1           1  1.jpg    1      golf_ball\n",
            "2           2  2.jpg    1    french_horn\n",
            "3           3  3.jpg    0      chain_saw\n",
            "4           4  4.jpg    0      parachute\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = AdversarialImageDataset(\n",
        "    csv_path=\"/content/10_Labels.csv\",\n",
        "    image_dir=\"/content/images\"\n",
        ")\n",
        "\n",
        "# Test the first and last image\n",
        "img_first, label_first = dataset[0]\n",
        "img_last, label_last = dataset[158]\n",
        "\n",
        "print(f\"First image shape: {img_first.shape}\") # Should be [3, 224, 224]\n",
        "print(f\"Last image (158) loaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHmcBkp-x3-Y",
        "outputId": "cc61b7ef-1343-454f-a091-eca9f9fc9f42"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First image shape: torch.Size([3, 224, 224])\n",
            "Last image (158) loaded successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "BATCH_SIZE = 64   # GPU available â†’ increase batch\n",
        "NUM_WORKERS = 2   # Colab safe\n",
        "\n",
        "train_size = int(0.8 * len(dataset))\n",
        "val_size = len(dataset) - train_size\n",
        "\n",
        "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    pin_memory=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "PFnyzGWBgVaE"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model = models.resnet18(weights=\"IMAGENET1K_V1\")\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "model = model.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YpBI1GE5grO9",
        "outputId": "f33e0f8f-a5d7-43f2-ae14-2cf322a6c906"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "scaler = GradScaler()\n"
      ],
      "metadata": {
        "id": "qFqpgfOeq6fi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e401c9bb-6e08-4fde-acb7-aa3a3c30d927"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-178857144.py:3: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/usr/local/lib/python3.12/dist-packages/torch/cuda/amp/grad_scaler.py:31: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  super().__init__(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "EPOCHS = 5\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\"):\n",
        "        imgs = imgs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():  # ðŸ”¥ mixed precision\n",
        "            outputs = model(imgs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1} | Train Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "PUvaUxzirJCr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0636fce-622d-425f-9bb6-b4f47d5d68af"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEpoch 1/5:   0%|          | 0/2 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/tmp/ipython-input-453357953.py:15: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():  # ðŸ”¥ mixed precision\n",
            "/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py:270: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n",
            "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:37<00:00, 18.76s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 0.7278\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:32<00:00, 16.50s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 | Train Loss: 0.7306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:33<00:00, 16.71s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 | Train Loss: 0.7315\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:34<00:00, 17.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 | Train Loss: 0.7286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:32<00:00, 16.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 | Train Loss: 0.7314\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "model.eval()\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "y_scores = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        imgs = imgs.to(device)\n",
        "        outputs = model(imgs)\n",
        "\n",
        "        probs = torch.softmax(outputs, dim=1)[:, 1].cpu().numpy()\n",
        "        preds = (probs > 0.5).astype(int)\n",
        "\n",
        "        y_true.extend(labels.numpy())\n",
        "        y_pred.extend(preds)\n",
        "        y_scores.extend(probs)\n",
        "\n",
        "print(\"Precision :\", round(precision_score(y_true, y_pred), 3))\n",
        "print(\"Recall    :\", round(recall_score(y_true, y_pred), 3))\n",
        "print(\"F1 Score  :\", round(f1_score(y_true, y_pred), 3))\n",
        "print(\"ROC AUC   :\", round(roc_auc_score(y_true, y_scores), 3))\n"
      ],
      "metadata": {
        "id": "tD6dJqjmrLxO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a81c4c7-9224-47bd-8658-4dd098758067"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision : 0.5\n",
            "Recall    : 0.467\n",
            "F1 Score  : 0.483\n",
            "ROC AUC   : 0.537\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "\n",
        "best_idx = np.argmax(tpr - fpr)   # Youdenâ€™s J statistic\n",
        "best_thresh = thresholds[best_idx]\n",
        "\n",
        "print(\"Best threshold:\", best_thresh)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giE9iHrly9SO",
        "outputId": "b4b77ea1-f423-42f9-f4ca-eaa903789aff"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best threshold: 0.35139182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "\n",
        "best_idx = np.argmax(tpr - fpr)   # Youdenâ€™s J statistic\n",
        "best_thresh = thresholds[best_idx]\n",
        "\n",
        "print(\"Best threshold:\", best_thresh)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bcv3CFP2zs5v",
        "outputId": "5c85f5b7-c13a-42b8-d952-1c026a1f584c"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best threshold: 0.35139182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = (np.array(y_scores) > best_thresh).astype(int)\n",
        "\n",
        "print(\"Precision :\", precision_score(y_true, y_pred))\n",
        "print(\"Recall    :\", recall_score(y_true, y_pred))\n",
        "print(\"F1 Score  :\", f1_score(y_true, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3R0GLhdTzw50",
        "outputId": "1e0bdf03-0e05-454b-dde1-7a69f25a231d"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision : 0.5454545454545454\n",
            "Recall    : 0.8\n",
            "F1 Score  : 0.6486486486486487\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "classes = np.unique(y_true)\n",
        "weights = compute_class_weight(\"balanced\", classes=classes, y=y_true)\n",
        "class_weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n"
      ],
      "metadata": {
        "id": "0M3LSLxNz0u4"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if \"layer4\" in name or \"fc\" in name:\n",
        "        param.requires_grad = True\n",
        "    else:\n",
        "        param.requires_grad = False\n"
      ],
      "metadata": {
        "id": "Ko30XkQmz7Hb"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=3e-4\n",
        ")\n"
      ],
      "metadata": {
        "id": "gntZfks60FK8"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(0.2,0.2,0.2,0.1),\n",
        "    transforms.RandomGrayscale(p=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485,0.456,0.406],\n",
        "        std=[0.229,0.224,0.225]\n",
        "    )\n",
        "])\n"
      ],
      "metadata": {
        "id": "oj7drstT0IlM"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"image_adversarial_cnn.pt\")\n",
        "print(\"âœ… Model saved\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EsoKFxiM6B18",
        "outputId": "c0614767-8186-4408-bb8d-9c4b7a584574"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "# same transform used in training\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "def test_single_image(image_path, model, device):\n",
        "    model.eval()\n",
        "\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = test_transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image)\n",
        "        probs = torch.softmax(output, dim=1)[0]\n",
        "\n",
        "    clean_prob = probs[0].item()\n",
        "    adv_prob = probs[1].item()\n",
        "\n",
        "    # ðŸ”¥ Decision logic (hackathon-friendly)\n",
        "    if adv_prob > 0.7:\n",
        "        action = \"BLOCK\"\n",
        "        label = \"ADVERSARIAL\"\n",
        "        confidence = adv_prob\n",
        "    elif adv_prob > 0.4:\n",
        "        action = \"WARNING\"\n",
        "        label = \"SUSPICIOUS\"\n",
        "        confidence = adv_prob\n",
        "    else:\n",
        "        action = \"ALLOW\"\n",
        "        label = \"CLEAN\"\n",
        "        confidence = clean_prob\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(\"Image      :\", image_path)\n",
        "    print(\"Prediction :\", label)\n",
        "    print(\"Confidence :\", f\"{confidence*100:.2f}%\")\n",
        "    print(\"Action     :\", action)\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    return label, confidence, action\n"
      ],
      "metadata": {
        "id": "BNgkl68p0N7J"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = models.resnet18(weights=None)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "\n",
        "model.load_state_dict(torch.load(\"image_adversarial_cnn.pt\", map_location=device))\n",
        "model.to(device).eval()\n",
        "\n",
        "print(\"Model loaded on:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_lKsZBg5DVz",
        "outputId": "730a25c1-3022-4459-857b-87ee5eaac804"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model = models.resnet18(weights=None)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)\n",
        "\n",
        "model.load_state_dict(torch.load(\"image_adversarial_cnn.pt\", map_location=device))\n",
        "model.to(device).eval()\n",
        "\n",
        "print(\"Model loaded on:\", device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KO2j4jk6KO-",
        "outputId": "53c08fbc-e419-410f-bcc1-94ae5da1f037"
      },
      "execution_count": 125,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded on: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "def test_image_colab(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = test_transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        probs = torch.softmax(model(image), dim=1)[0]\n",
        "\n",
        "    clean_prob = probs[0].item()\n",
        "    adv_prob = probs[1].item()\n",
        "\n",
        "    if adv_prob > 0.7:\n",
        "        action = \"BLOCK\"\n",
        "        label = \"ADVERSARIAL\"\n",
        "        conf = adv_prob\n",
        "    elif adv_prob > 0.4:\n",
        "        action = \"WARNING\"\n",
        "        label = \"SUSPICIOUS\"\n",
        "        conf = adv_prob\n",
        "    else:\n",
        "        action = \"ALLOW\"\n",
        "        label = \"CLEAN\"\n",
        "        conf = clean_prob\n",
        "\n",
        "    print(\"=\"*50)\n",
        "    print(\"Image:\", image_path)\n",
        "    print(\"Prediction:\", label)\n",
        "    print(\"Confidence:\", f\"{conf*100:.2f}%\")\n",
        "    print(\"Action:\", action)\n",
        "    print(\"=\"*50)\n"
      ],
      "metadata": {
        "id": "iG2pQVpP6N0i"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_image_colab(\"/content/images/10.jpg\")\n",
        "test_image_colab(\"/content/images/50.jpg\")\n",
        "test_image_colab(\"/content/images/120.jpg\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9UBv9Ke6S2k",
        "outputId": "c90a654a-4639-4a97-d7c9-b54b8b83563c"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "Image: /content/images/10.jpg\n",
            "Prediction: CLEAN\n",
            "Confidence: 66.30%\n",
            "Action: ALLOW\n",
            "==================================================\n",
            "==================================================\n",
            "Image: /content/images/50.jpg\n",
            "Prediction: SUSPICIOUS\n",
            "Confidence: 43.87%\n",
            "Action: WARNING\n",
            "==================================================\n",
            "==================================================\n",
            "Image: /content/images/120.jpg\n",
            "Prediction: CLEAN\n",
            "Confidence: 64.97%\n",
            "Action: ALLOW\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save({\n",
        "    \"model_state\": model.state_dict(),\n",
        "    \"model_name\": \"resnet18\",\n",
        "    \"num_classes\": 2\n",
        "}, \"image_adversarial_model.pt\")\n",
        "\n",
        "print(\"Model saved\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SadfY1i6Yeb",
        "outputId": "ea354712-50b9-4cfc-f592-896d7cb2048f"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3PQ9jA_x64Aj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}